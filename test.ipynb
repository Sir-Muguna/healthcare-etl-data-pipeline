{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'npi_registry_results.csv' has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Define the base URL of the API\n",
    "api_url = \"https://npiregistry.cms.hhs.gov/api/\"\n",
    "\n",
    "# Set parameters (adjust limit and skip for pagination)\n",
    "params = {\n",
    "    \"number\": \"\",\n",
    "    \"enumeration_type\": \"\",\n",
    "    \"taxonomy_description\": \"\",\n",
    "    \"name_purpose\": \"\",\n",
    "    \"first_name\": \"\",\n",
    "    \"use_first_name_alias\": \"\",\n",
    "    \"last_name\": \"\",\n",
    "    \"organization_name\": \"\",\n",
    "    \"address_purpose\": \"\",\n",
    "    \"city\": \"Topeka\",\n",
    "    \"state\": \"KS\",  # Corrected to the two-letter state code\n",
    "    \"postal_code\": \"\",\n",
    "    \"country_code\": \"\",\n",
    "    \"limit\": 200,\n",
    "    \"skip\": 0,\n",
    "    \"pretty\": \"on\",\n",
    "    \"version\": \"2.1\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Make the API request with a timeout\n",
    "    response = requests.get(api_url, params=params, timeout=10)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON data\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract results from JSON data\n",
    "        results = data.get(\"results\", [])\n",
    "\n",
    "        if results:\n",
    "            # Define CSV file path\n",
    "            csv_filename = \"npi_registry_results.csv\"\n",
    "\n",
    "            # Extract relevant fields from each result\n",
    "            csv_data = []\n",
    "            for result in results:\n",
    "                row = {\n",
    "                    \"number\": result.get(\"number\", \"\"),\n",
    "                    \"enumeration_type\": result.get(\"enumeration_type\", \"\"),\n",
    "                    \"enumeration_date\": result.get(\"enumeration_date\", \"\"),\n",
    "                    \"taxonomy_description\": result.get(\"taxonomy_description\", \"\"),\n",
    "                    \"first_name\": result.get(\"basic\", {}).get(\"first_name\", \"\"),\n",
    "                    \"last_name\": result.get(\"basic\", {}).get(\"last_name\", \"\"),\n",
    "                    \"organization_name\": result.get(\"basic\", {}).get(\"organization_name\", \"\"),\n",
    "                    \"address\": result.get(\"addresses\", [{}])[0].get(\"address_1\", \"\"),\n",
    "                    \"address_purpose\": result.get(\"basic\", {}).get(\"address_purpose\", \"\"),\n",
    "                    \"city\": result.get(\"addresses\", [{}])[0].get(\"city\", \"\"),\n",
    "                    \"state\": result.get(\"addresses\", [{}])[0].get(\"state\", \"\"),\n",
    "                    \"postal_code\": result.get(\"addresses\", [{}])[0].get(\"postal_code\", \"\"),\n",
    "                    \"country_code\": result.get(\"addresses\", [{}])[0].get(\"country_code\", \"\"),\n",
    "                    \"telephone_number\": result.get(\"addresses\", [{}])[0].get(\"telephone_number\", \"\"),\n",
    "                    \"fax_number\": result.get(\"addresses\", [{}])[0].get(\"fax_number\", \"\"),\n",
    "                    \"taxonomy_code\": result.get(\"taxonomies\", [{}])[0].get(\"code\", \"\"),\n",
    "                    \"taxonomy_group\": result.get(\"taxonomies\", [{}])[0].get(\"taxonomy_group\", \"\"),\n",
    "                    \"taxonomy_description\": result.get(\"taxonomies\", [{}])[0].get(\"desc\", \"\"),\n",
    "                    \"license_no\": result.get(\"taxonomies\", [{}])[0].get(\"license\", \"\")\n",
    "                }\n",
    "                csv_data.append(row)\n",
    "\n",
    "            # Write data to CSV file\n",
    "            with open(csv_filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = [\"number\", \"enumeration_type\", \"enumeration_date\",\"taxonomy_description\", \"first_name\", \"last_name\",\n",
    "                              \"organization_name\", \"address\", \"address_purpose\",\"city\", \"state\", \"postal_code\", \"country_code\",\n",
    "                              \"telephone_number\", \"fax_number\", \"taxonomy_code\", \"taxonomy_group\", \"taxonomy_description\", \n",
    "                              \"license_no\"\n",
    "                              ]\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "                writer.writeheader()\n",
    "                for row in csv_data:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "            print(f\"CSV file '{csv_filename}' has been successfully created.\")\n",
    "        else:\n",
    "            print(\"No results found.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        print(f\"Error message: {response.text}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the ZIP archive:\n",
      "HCPC2024_JUL_ANWEB_Transaction Report_v3.xlsx\n",
      "HCPC2024_JUL_ANWEB_v3.txt\n",
      "HCPC2024_JUL_ANWEB_v3.xlsx\n",
      "HCPC2024_JUL_Corrections on V3.xlsx\n",
      "HCPC2024_recordlayout.txt\n",
      "NOC codes_JUL 2024.xlsx\n",
      "proc_notes_JUL2024.txt\n",
      "'NOC codes_JUL 2024.xlsx' has been extracted successfully to '/root/projects/portfolio_projects/data_engineering/healthcare-etl-data-pipeline/data_extraction/files'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "\n",
    "def get_quarter_and_abbr(month):\n",
    "    \"\"\"Determine the quarter and its corresponding abbreviation based on the month.\"\"\"\n",
    "    if month in [1, 2, 3]:\n",
    "        return \"january\", \"JAN\"\n",
    "    elif month in [4, 5, 6]:\n",
    "        return \"april\", \"APR\"\n",
    "    elif month in [7, 8, 9]:\n",
    "        return \"july\", \"JUL\"\n",
    "    else:\n",
    "        return \"october\", \"OCT\"\n",
    "\n",
    "def get_quarterly_zip_url(year=None, month=None):\n",
    "    \"\"\"Construct the quarterly ZIP URL based on the current year and quarter.\"\"\"\n",
    "    now = datetime.now()\n",
    "    year = year or now.year\n",
    "    month = month or now.month\n",
    "\n",
    "    # Get the quarter name\n",
    "    quarter, _ = get_quarter_and_abbr(month)\n",
    "\n",
    "    # Construct the URL\n",
    "    zip_url = f\"https://www.cms.gov/files/zip/{quarter}-{year}-alpha-numeric-hcpcs-file.zip\"\n",
    "    return zip_url\n",
    "\n",
    "def list_files_in_zip(zip_url):\n",
    "    \"\"\"Download the ZIP file and list all files inside.\"\"\"\n",
    "    try:\n",
    "        # Send a GET request to download the ZIP file content\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Create a BytesIO object from the response content\n",
    "            zip_file = BytesIO(response.content)\n",
    "\n",
    "            # Open the ZIP file from the BytesIO object\n",
    "            with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "                # List all files inside the ZIP\n",
    "                print(\"Files in the ZIP archive:\")\n",
    "                for file in z.namelist():\n",
    "                    print(file)\n",
    "                return z.namelist()  # Return the list of files\n",
    "        else:\n",
    "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "            print(f\"Error message: {response.text}\")\n",
    "            return []\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "        return []\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"The downloaded file is not a valid ZIP file.\")\n",
    "        return []\n",
    "\n",
    "def download_and_extract_specific_file(zip_url, filename_prefix, extract_to):\n",
    "    \"\"\"Download the ZIP file and extract the specific NOC file (.xls or .xlsx) to the given path.\"\"\"\n",
    "    try:\n",
    "        # Send a GET request to download the ZIP file content\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Create a BytesIO object from the response content\n",
    "            zip_file = BytesIO(response.content)\n",
    "\n",
    "            # Open the ZIP file from the BytesIO object\n",
    "            with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "                # Try to find the file with either .xls or .xlsx extension\n",
    "                file_to_extract = None\n",
    "                for file in z.namelist():\n",
    "                    if file.startswith(filename_prefix) and (file.endswith('.xls') or file.endswith('.xlsx')):\n",
    "                        file_to_extract = file\n",
    "                        break\n",
    "\n",
    "                # If the file was found, extract it\n",
    "                if file_to_extract:\n",
    "                    z.extract(file_to_extract, extract_to)\n",
    "                    print(f\"'{file_to_extract}' has been extracted successfully to '{extract_to}'.\")\n",
    "                else:\n",
    "                    print(f\"'{filename_prefix}' not found in the ZIP file with either .xls or .xlsx extension.\")\n",
    "        else:\n",
    "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "            print(f\"Error message: {response.text}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"The downloaded file is not a valid ZIP file.\")\n",
    "\n",
    "def get_noc_codes_filename_prefix(year, quarter_abbr):\n",
    "    \"\"\"Construct the expected NOC codes file prefix with a space between the month and year.\"\"\"\n",
    "    return f\"NOC codes_{quarter_abbr} {year}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the current date and determine the year and month\n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "\n",
    "    # Get the current quarter and its abbreviation\n",
    "    quarter, quarter_abbr = get_quarter_and_abbr(month)\n",
    "\n",
    "    # Get the URL for the quarterly ZIP file\n",
    "    zip_url = get_quarterly_zip_url(year, month)\n",
    "\n",
    "    # List all files inside the ZIP archive\n",
    "    zip_file_list = list_files_in_zip(zip_url)\n",
    "\n",
    "    # Define the expected filename prefix for the NOC codes file (with space between month and year)\n",
    "    noc_filename_prefix = get_noc_codes_filename_prefix(year, quarter_abbr)\n",
    "\n",
    "    # Define the path to save the extracted file\n",
    "    extract_path = \"/root/projects/portfolio_projects/data_engineering/healthcare-etl-data-pipeline/data_extraction/files\"\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    # Extract the NOC codes file if it exists in the ZIP\n",
    "    download_and_extract_specific_file(zip_url, noc_filename_prefix, extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICD-10 is available at https://www.cms.gov/files/document/valid-icd-10-list.xlsx\n",
      "ICD-11 not found. Latest available version is ICD-10.\n",
      "File 'valid-icd-10-list.xlsx' downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Base URL format for the ICD files\n",
    "BASE_URL = \"https://www.cms.gov/files/document/valid-icd-{}-list.xlsx\"\n",
    "\n",
    "def check_version_exists(version):\n",
    "    \"\"\"Check if a specific version of the ICD list exists.\"\"\"\n",
    "    url = BASE_URL.format(version)\n",
    "    response = requests.head(url)\n",
    "    if response.status_code == 200:\n",
    "        return True, url\n",
    "    else:\n",
    "        return False, None\n",
    "\n",
    "def get_latest_icd_version():\n",
    "    \"\"\"Determine the latest available ICD version by incrementing version numbers.\"\"\"\n",
    "    version = 10  # Start checking from ICD-10\n",
    "\n",
    "    while True:\n",
    "        # Check if the next version exists\n",
    "        exists, url = check_version_exists(version)\n",
    "        if exists:\n",
    "            latest_version = version\n",
    "            print(f\"ICD-{latest_version} is available at {url}\")\n",
    "            version += 1  # Check the next version\n",
    "        else:\n",
    "            # If the next version does not exist, return the latest available version\n",
    "            print(f\"ICD-{version} not found. Latest available version is ICD-{latest_version}.\")\n",
    "            return latest_version, BASE_URL.format(latest_version)\n",
    "\n",
    "def download_icd_file(icd_url):\n",
    "    \"\"\"Download the ICD file from the provided URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(icd_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the filename from the URL\n",
    "            file_name = icd_url.split('/')[-1]\n",
    "\n",
    "            # Save the file\n",
    "            with open(file_name, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            print(f\"File '{file_name}' downloaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the latest ICD version URL\n",
    "    latest_version, latest_icd_url = get_latest_icd_version()\n",
    "\n",
    "    # If a valid URL was found, download the ICD file\n",
    "    if latest_icd_url:\n",
    "        download_icd_file(latest_icd_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICD-10 is available at https://www.cms.gov/files/document/valid-icd-10-list.xlsx\n",
      "ICD-11 not found. Latest available version is ICD-10.\n",
      "File 'valid-icd-10-list.xlsx' downloaded successfully.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=0\n",
      "Fetched 1000 records, Total so far: 1000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=1000\n",
      "Fetched 1000 records, Total so far: 2000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=2000\n",
      "Fetched 1000 records, Total so far: 3000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=3000\n",
      "Fetched 1000 records, Total so far: 4000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=4000\n",
      "Fetched 1000 records, Total so far: 5000\n",
      "Data saved to /root/projects/portfolio_projects/data_engineering/healthcare-etl-data-pipeline/data_extraction/files/ndc_finished_drug_products.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# Base API URL\n",
    "BASE_API_URL = \"https://api.fda.gov/drug/ndc.json\"\n",
    "\n",
    "def fetch_fda_data(base_url, query, limit=1000, max_records=10000):\n",
    "    all_data = []\n",
    "    skip = 0\n",
    "\n",
    "    while True:\n",
    "        # Construct the full URL with limit and skip for pagination\n",
    "        url = f\"{base_url}?search={query}&limit={limit}&skip={skip}\"\n",
    "        print(f\"Fetching data from: {url}\")\n",
    "        \n",
    "        try:\n",
    "            # Send a GET request to the API\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "                \n",
    "                # Check if there are any results\n",
    "                if \"results\" in data and data[\"results\"]:\n",
    "                    all_data.extend(data[\"results\"])\n",
    "                    print(f\"Fetched {len(data['results'])} records, Total so far: {len(all_data)}\")\n",
    "                    \n",
    "                    # Increase skip to get the next batch of records\n",
    "                    skip += limit\n",
    "                    \n",
    "                    # Stop if we've fetched at least 5000 records\n",
    "                    if len(all_data) >= max_records:\n",
    "                        all_data = all_data[:max_records]  # Limit to exactly 5000 records if more were fetched\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"No more data to fetch.\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def save_data_to_csv(data, file_path):\n",
    "    try:\n",
    "        # Open a file for writing\n",
    "        with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the header row\n",
    "            writer.writerow([\n",
    "                \"product_ndc\", \"generic_name\", \"labeler_name\", \"brand_name\", \n",
    "                \"active_ingredient_name\", \"active_ingredient_strength\", \"finished\", \n",
    "                \"package_ndc\", \"package_description\", \"marketing_start_date\", \n",
    "                \"listing_expiration_date\", \"manufacturer_name\", \"marketing_category\", \n",
    "                \"dosage_form\", \"product_type\", \"route\"\n",
    "            ])\n",
    "            \n",
    "            # Write the data rows\n",
    "            for item in data:\n",
    "                # Some fields have nested data (active_ingredients, packaging, openfda), we need to handle these\n",
    "                product_ndc = item.get('product_ndc', '')\n",
    "                generic_name = item.get('generic_name', '')\n",
    "                labeler_name = item.get('labeler_name', '')\n",
    "                brand_name = item.get('brand_name', '')\n",
    "                finished = item.get('finished', '')\n",
    "                listing_expiration_date = item.get('listing_expiration_date', '')\n",
    "                marketing_category = item.get('marketing_category', '')\n",
    "                dosage_form = item.get('dosage_form', '')\n",
    "                product_type = item.get('product_type', '')\n",
    "                \n",
    "                # Extract active ingredient (handling multiple active ingredients)\n",
    "                for active in item.get('active_ingredients', []):\n",
    "                    active_ingredient_name = active.get('name', '')\n",
    "                    active_ingredient_strength = active.get('strength', '')\n",
    "                    \n",
    "                    # Extract packaging details (handling multiple packages)\n",
    "                    for pkg in item.get('packaging', []):\n",
    "                        package_ndc = pkg.get('package_ndc', '')\n",
    "                        package_description = pkg.get('description', '')\n",
    "                        marketing_start_date = pkg.get('marketing_start_date', '')\n",
    "                        \n",
    "                        # Extract manufacturer_name from openfda\n",
    "                        manufacturer_name = ','.join(item.get('openfda', {}).get('manufacturer_name', []))\n",
    "                        \n",
    "                        # Extract route (can be multiple)\n",
    "                        route = ','.join(item.get('route', []))\n",
    "\n",
    "                        # Write row with all fields\n",
    "                        writer.writerow([\n",
    "                            product_ndc, generic_name, labeler_name, brand_name, \n",
    "                            active_ingredient_name, active_ingredient_strength, finished, \n",
    "                            package_ndc, package_description, marketing_start_date, \n",
    "                            listing_expiration_date, manufacturer_name, marketing_category, \n",
    "                            dosage_form, product_type, route\n",
    "                        ])\n",
    "        \n",
    "        print(f\"Data saved to {file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving data to file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Search query for finished drugs\n",
    "    query = \"finished:true\"\n",
    "    \n",
    "    # Fetch all data from the FDA API with a limit of 5000 records\n",
    "    fda_data = fetch_fda_data(BASE_API_URL, query, limit=1000, max_records=10000)\n",
    "    \n",
    "    # If data was successfully retrieved, save it to a CSV file\n",
    "    if fda_data:\n",
    "        # Define the file path where the data will be saved as CSV\n",
    "        file_path = \"/root/projects/portfolio_projects/data_engineering/healthcare-etl-data-pipeline/data_extraction/files/fda_ndc_raw.csv\"\n",
    "        save_data_to_csv(fda_data, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICD-10 is available at https://www.cms.gov/files/document/valid-icd-10-list.xlsx\n",
      "ICD-11 not found. Latest available version is ICD-10.\n",
      "Metadata for ICD-10 saved to database.\n",
      "Current records in the table: [(1, 10, 'valid-icd-10-list.xlsx', 'https://www.cms.gov/files/document/valid-icd-10-list.xlsx')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "BASE_URL = \"https://www.cms.gov/files/document/valid-icd-{}-list.xlsx\"\n",
    "\n",
    "db_params = {\n",
    "    'dbname': 'health_data',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "def check_version_exists(version):\n",
    "    url = BASE_URL.format(version)\n",
    "    response = requests.head(url)\n",
    "    return response.status_code == 200, url\n",
    "\n",
    "def get_latest_icd_version():\n",
    "    version = 10\n",
    "    latest_version = None\n",
    "\n",
    "    while True:\n",
    "        exists, url = check_version_exists(version)\n",
    "        if exists:\n",
    "            latest_version = version\n",
    "            print(f\"ICD-{latest_version} is available at {url}\")\n",
    "            version += 1\n",
    "        else:\n",
    "            if latest_version is not None:\n",
    "                print(f\"ICD-{version} not found. Latest available version is ICD-{latest_version}.\")\n",
    "                return latest_version, BASE_URL.format(latest_version)\n",
    "            else:\n",
    "                print(\"No valid ICD versions found.\")\n",
    "                return None, None\n",
    "\n",
    "def insert_icd_metadata_to_db(version, file_name, url):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS icd_files (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            version INT,\n",
    "            file_name VARCHAR(255),\n",
    "            file_url TEXT\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO icd_files (version, file_name, file_url)\n",
    "        VALUES (%s, %s, %s);\n",
    "        \"\"\"\n",
    "        cursor.execute(insert_query, (version, file_name, url))\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Metadata for ICD-{version} saved to database.\")\n",
    "\n",
    "        # Confirm the insertion\n",
    "        cursor.execute(\"SELECT * FROM icd_files WHERE version = %s;\", (version,))\n",
    "        records = cursor.fetchall()\n",
    "        print(\"Current records in the table:\", records)\n",
    "\n",
    "    except psycopg2.DatabaseError as db_error:\n",
    "        print(f\"Database error: {db_error}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "def download_icd_file(icd_url, version):\n",
    "    try:\n",
    "        response = requests.get(icd_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            file_name = icd_url.split('/')[-1]\n",
    "            insert_icd_metadata_to_db(version, file_name, icd_url)\n",
    "        else:\n",
    "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    latest_version, latest_icd_url = get_latest_icd_version()\n",
    "    if latest_icd_url:\n",
    "        download_icd_file(latest_icd_url, latest_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('/root/projects/portfolio_projects/data_engineering/healthcare-etl-data-pipeline/icd_codes_data.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for line_number, row in enumerate(reader, start=1):\n",
    "        if len(row) != 3:\n",
    "            print(f\"Line {line_number} has {len(row)} columns: {row}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=0\n",
      "Fetched 1000 records, Total so far: 1000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=1000\n",
      "Fetched 1000 records, Total so far: 2000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=2000\n",
      "Fetched 1000 records, Total so far: 3000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=3000\n",
      "Fetched 1000 records, Total so far: 4000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=4000\n",
      "Fetched 1000 records, Total so far: 5000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=5000\n",
      "Fetched 1000 records, Total so far: 6000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=6000\n",
      "Fetched 1000 records, Total so far: 7000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=7000\n",
      "Fetched 1000 records, Total so far: 8000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=8000\n",
      "Fetched 1000 records, Total so far: 9000\n",
      "Fetching data from: https://api.fda.gov/drug/ndc.json?search=finished:true&limit=1000&skip=9000\n",
      "Fetched 1000 records, Total so far: 10000\n",
      "Data saved to PostgreSQL successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "# Base API URL\n",
    "BASE_API_URL = \"https://api.fda.gov/drug/ndc.json\"\n",
    "\n",
    "db_params = {\n",
    "    'host': 'localhost', \n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'options': '-c search_path=private'\n",
    "}\n",
    "\n",
    "def fetch_fda_data(base_url, query, limit=1000, max_records=10000):\n",
    "    all_data = []\n",
    "    skip = 0\n",
    "\n",
    "    while True:\n",
    "        # Construct the full URL with limit and skip for pagination\n",
    "        url = f\"{base_url}?search={query}&limit={limit}&skip={skip}\"\n",
    "        print(f\"Fetching data from: {url}\")\n",
    "        \n",
    "        try:\n",
    "            # Send a GET request to the API\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "                \n",
    "                # Check if there are any results\n",
    "                if \"results\" in data and data[\"results\"]:\n",
    "                    all_data.extend(data[\"results\"])\n",
    "                    print(f\"Fetched {len(data['results'])} records, Total so far: {len(all_data)}\")\n",
    "                    \n",
    "                    # Increase skip to get the next batch of records\n",
    "                    skip += limit\n",
    "                    \n",
    "                    # Stop if we've fetched at least 5000 records\n",
    "                    if len(all_data) >= max_records:\n",
    "                        all_data = all_data[:max_records]  # Limit to exactly 5000 records if more were fetched\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"No more data to fetch.\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def save_data_to_postgres(data, db_params):\n",
    "    try:\n",
    "        # Establish a connection to PostgreSQL\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table if it doesn't exist\n",
    "        create_table_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS fda_ndc_raw (\n",
    "            product_ndc VARCHAR(50),\n",
    "            generic_name VARCHAR(255),\n",
    "            labeler_name VARCHAR(255),\n",
    "            brand_name VARCHAR(255),\n",
    "            active_ingredient_name VARCHAR(255),\n",
    "            active_ingredient_strength VARCHAR(255),\n",
    "            finished BOOLEAN,\n",
    "            package_ndc VARCHAR(50),\n",
    "            package_description TEXT,\n",
    "            marketing_start_date DATE,\n",
    "            listing_expiration_date DATE,\n",
    "            manufacturer_name VARCHAR(255),\n",
    "            marketing_category VARCHAR(255),\n",
    "            dosage_form VARCHAR(100),\n",
    "            product_type VARCHAR(100),\n",
    "            route VARCHAR(255)\n",
    "        );\n",
    "        '''\n",
    "        cursor.execute(create_table_query)\n",
    "        \n",
    "        # Insert data\n",
    "        insert_query = '''\n",
    "        INSERT INTO fda_ndc_raw (\n",
    "            product_ndc, generic_name, labeler_name, brand_name, active_ingredient_name,\n",
    "            active_ingredient_strength, finished, package_ndc, package_description, \n",
    "            marketing_start_date, listing_expiration_date, manufacturer_name, \n",
    "            marketing_category, dosage_form, product_type, route\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "        '''\n",
    "        \n",
    "        for item in data:\n",
    "            product_ndc = item.get('product_ndc', '')\n",
    "            generic_name = item.get('generic_name', '')\n",
    "            labeler_name = item.get('labeler_name', '')\n",
    "            brand_name = item.get('brand_name', '')\n",
    "            finished = item.get('finished', False)\n",
    "            listing_expiration_date = item.get('listing_expiration_date', None)\n",
    "            marketing_category = item.get('marketing_category', '')\n",
    "            dosage_form = item.get('dosage_form', '')\n",
    "            product_type = item.get('product_type', '')\n",
    "            \n",
    "            for active in item.get('active_ingredients', []):\n",
    "                active_ingredient_name = active.get('name', '')\n",
    "                active_ingredient_strength = active.get('strength', '')\n",
    "                \n",
    "                for pkg in item.get('packaging', []):\n",
    "                    package_ndc = pkg.get('package_ndc', '')\n",
    "                    package_description = pkg.get('description', '')\n",
    "                    marketing_start_date = pkg.get('marketing_start_date', None)\n",
    "                    \n",
    "                    manufacturer_name = ','.join(item.get('openfda', {}).get('manufacturer_name', []))\n",
    "                    route = ','.join(item.get('route', []))\n",
    "\n",
    "                    # Insert the row into PostgreSQL\n",
    "                    cursor.execute(insert_query, (\n",
    "                        product_ndc, generic_name, labeler_name, brand_name, \n",
    "                        active_ingredient_name, active_ingredient_strength, finished, \n",
    "                        package_ndc, package_description, marketing_start_date, \n",
    "                        listing_expiration_date, manufacturer_name, marketing_category, \n",
    "                        dosage_form, product_type, route\n",
    "                    ))\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Data saved to PostgreSQL successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to PostgreSQL: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Search query for finished drugs\n",
    "    query = \"finished:true\"\n",
    "    \n",
    "    # Fetch all data from the FDA API with a limit of 5000 records\n",
    "    fda_data = fetch_fda_data(BASE_API_URL, query, limit=1000, max_records=10000)\n",
    "    \n",
    "    # If data was successfully retrieved, save it to PostgreSQL\n",
    "    if fda_data:\n",
    "        save_data_to_postgres(fda_data, db_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File npi_registry_raw.json successfully saved to GCS bucket health_data_pipeline\n",
      "File npi_registry_raw.csv successfully saved to GCS bucket health_data_pipeline\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "# Set the environment variable for Google Cloud credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/root/projects/portfolio_projects/data_engineering/healthcare-etl-data-pipeline/gcp/healthcare-etl-data-project-0499b67cb8ee.json\"\n",
    "\n",
    "\n",
    "# Define the base URL of the API\n",
    "api_url = \"https://npiregistry.cms.hhs.gov/api/\"\n",
    "\n",
    "# Set parameters for API requests\n",
    "params = {\n",
    "    \"number\": \"\",\n",
    "    \"enumeration_type\": \"\",\n",
    "    \"taxonomy_description\": \"\",\n",
    "    \"name_purpose\": \"\",\n",
    "    \"first_name\": \"\",\n",
    "    \"use_first_name_alias\": \"\",\n",
    "    \"last_name\": \"\",\n",
    "    \"organization_name\": \"\",\n",
    "    \"address_purpose\": \"\",\n",
    "    \"city\": \"Topeka\",\n",
    "    \"state\": \"KS\",\n",
    "    \"postal_code\": \"\",\n",
    "    \"country_code\": \"\",\n",
    "    \"limit\": 200,\n",
    "    \"skip\": 0,\n",
    "    \"pretty\": \"on\",\n",
    "    \"version\": \"2.1\"\n",
    "}\n",
    "\n",
    "def fetch_npi_data(params):\n",
    "    try:\n",
    "        response = requests.get(api_url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get(\"results\", [])\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_gcs(bucket_name, file_name, data, content_type='application/json'):\n",
    "    \"\"\"Saves data to Google Cloud Storage.\"\"\"\n",
    "    try:\n",
    "        # Initialize the Google Cloud Storage client\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "        \n",
    "        # Upload data to GCS\n",
    "        if content_type == 'application/json':\n",
    "            blob.upload_from_string(data, content_type='application/json')\n",
    "        elif content_type == 'text/csv':\n",
    "            blob.upload_from_string(data, content_type='text/csv')\n",
    "        \n",
    "        print(f\"File {file_name} successfully saved to GCS bucket {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save file to GCS: {e}\")\n",
    "\n",
    "def write_data_to_csv(data):\n",
    "    \"\"\"Writes API data to CSV format and returns it as a string.\"\"\"\n",
    "    output = []\n",
    "    for result in data:\n",
    "        row = {\n",
    "            'number': result.get(\"number\", \"\"),\n",
    "            'enumeration_type': result.get(\"enumeration_type\", \"\"),\n",
    "            'enumeration_date': result.get(\"basic\", {}).get(\"enumeration_date\", \"\"),\n",
    "            'first_name': result.get(\"basic\", {}).get(\"first_name\", \"\"),\n",
    "            'last_name': result.get(\"basic\", {}).get(\"last_name\", \"\"),\n",
    "            'organization_name': result.get(\"basic\", {}).get(\"organization_name\", \"\"),\n",
    "            'address': result.get(\"addresses\", [{}])[0].get(\"address_1\", \"\"),\n",
    "            'city': result.get(\"addresses\", [{}])[0].get(\"city\", \"\"),\n",
    "            'state': result.get(\"addresses\", [{}])[0].get(\"state\", \"\"),\n",
    "            'postal_code': result.get(\"addresses\", [{}])[0].get(\"postal_code\", \"\"),\n",
    "            'country_code': result.get(\"addresses\", [{}])[0].get(\"country_code\", \"\"),\n",
    "            'telephone_number': result.get(\"addresses\", [{}])[0].get(\"telephone_number\", \"\"),\n",
    "            'taxonomy_code': result.get(\"taxonomies\", [{}])[0].get(\"code\", \"\"),\n",
    "            'taxonomy_group': result.get(\"taxonomies\", [{}])[0].get(\"taxonomy_group\", \"\"),\n",
    "            'taxonomy_desc': result.get(\"taxonomies\", [{}])[0].get(\"desc\", \"\"),\n",
    "            'license_no': result.get(\"taxonomies\", [{}])[0].get(\"license\", \"\")\n",
    "        }\n",
    "        output.append(row)\n",
    "\n",
    "    # Write the data to CSV format\n",
    "    csv_output = csv.DictWriter(open('temp.csv', 'w'), fieldnames=output[0].keys())\n",
    "    csv_output.writeheader()\n",
    "    csv_output.writerows(output)\n",
    "\n",
    "    # Read the content of the CSV file back\n",
    "    with open('temp.csv', 'r') as f:\n",
    "        csv_data = f.read()\n",
    "    return csv_data\n",
    "\n",
    "def main():\n",
    "    # Fetch data from API\n",
    "    results = fetch_npi_data(params)\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "\n",
    "    # Convert the data to JSON format\n",
    "    json_data = json.dumps(results, indent=4)\n",
    "    file_name_json = 'npi_registry_raw.json'\n",
    "    \n",
    "    # Save the JSON data to GCS\n",
    "    save_to_gcs(bucket_name, file_name_json, json_data, content_type='application/json')\n",
    "\n",
    "    # Alternatively, write the data to CSV and save to GCS\n",
    "    csv_data = write_data_to_csv(results)\n",
    "    file_name_csv = 'npi_registry_raw.csv'\n",
    "    \n",
    "    # Save the CSV data to GCS\n",
    "    save_to_gcs(bucket_name, file_name_csv, csv_data, content_type='text/csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Database connection details (update these if needed)\n",
    "db_params = {\n",
    "    'host': 'localhost',\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'options': '-c search_path=private'\n",
    "}\n",
    "\n",
    "# Define the base URL of the API\n",
    "api_url = \"https://npiregistry.cms.hhs.gov/api/\"\n",
    "\n",
    "# Set parameters for API requests\n",
    "params = {\n",
    "    \"number\": \"\",\n",
    "    \"enumeration_type\": \"\",\n",
    "    \"taxonomy_description\": \"\",\n",
    "    \"name_purpose\": \"\",\n",
    "    \"first_name\": \"\",\n",
    "    \"use_first_name_alias\": \"\",\n",
    "    \"last_name\": \"\",\n",
    "    \"organization_name\": \"\",\n",
    "    \"address_purpose\": \"\",\n",
    "    \"city\": \"Topeka\",\n",
    "    \"state\": \"KS\",\n",
    "    \"postal_code\": \"\",\n",
    "    \"country_code\": \"\",\n",
    "    \"limit\": 200,\n",
    "    \"skip\": 0,\n",
    "    \"pretty\": \"on\",\n",
    "    \"version\": \"2.1\"\n",
    "}\n",
    "\n",
    "def create_npi_table():\n",
    "    \"\"\"Create the NPI registry table in PostgreSQL.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS npi_registry_raw (\n",
    "            number VARCHAR(50),\n",
    "            enumeration_type VARCHAR(50),\n",
    "            enumeration_date DATE,\n",
    "            taxonomy_description VARCHAR(255),\n",
    "            first_name VARCHAR(50),\n",
    "            last_name VARCHAR(50),\n",
    "            organization_name VARCHAR(255),\n",
    "            address VARCHAR(255),\n",
    "            address_purpose VARCHAR(255),\n",
    "            city VARCHAR(50),\n",
    "            state VARCHAR(2),\n",
    "            postal_code VARCHAR(20),\n",
    "            country_code VARCHAR(2),\n",
    "            telephone_number VARCHAR(20),\n",
    "            fax_number VARCHAR(20),\n",
    "            taxonomy_code VARCHAR(50),\n",
    "            taxonomy_group VARCHAR(255),\n",
    "            taxonomy_desc VARCHAR(255),\n",
    "            license_no VARCHAR(50)\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(\"Table npi_registry_raw created or already exists.\")\n",
    "    except psycopg2.DatabaseError as db_error:\n",
    "        print(f\"Database error: {db_error}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "def fetch_npi_data(params):\n",
    "    \"\"\"Fetch NPI data from the API using the provided parameters.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(api_url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get(\"results\", [])\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def insert_npi_data(results):\n",
    "    \"\"\"Insert fetched NPI data into the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO npi_registry_raw (\n",
    "            number, enumeration_type, enumeration_date, taxonomy_description,\n",
    "            first_name, last_name, organization_name, address, address_purpose,\n",
    "            city, state, postal_code, country_code, telephone_number, fax_number,\n",
    "            taxonomy_code, taxonomy_group, taxonomy_desc, license_no\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "\n",
    "        for result in results:\n",
    "            row = (\n",
    "                result.get(\"number\", \"\"),\n",
    "                result.get(\"enumeration_type\", \"\"),\n",
    "                result.get(\"basic\", {}).get(\"enumeration_date\", None),\n",
    "                result.get(\"taxonomy_description\", \"\"),\n",
    "                result.get(\"basic\", {}).get(\"first_name\", \"\"),\n",
    "                result.get(\"basic\", {}).get(\"last_name\", \"\"),\n",
    "                result.get(\"basic\", {}).get(\"organization_name\", \"\"),\n",
    "                result.get(\"addresses\", [{}])[0].get(\"address_1\", \"\"),\n",
    "                result.get(\"basic\", {}).get(\"address_purpose\", \"\"),\n",
    "                result.get(\"addresses\", [{}])[0].get(\"city\", \"\"),\n",
    "                result.get(\"addresses\", [{}])[0].get(\"state\", \"\"),\n",
    "                result.get(\"addresses\", [{}])[0].get(\"postal_code\", \"\"),\n",
    "                result.get(\"addresses\", [{}])[0].get(\"country_code\", \"\"),\n",
    "                result.get(\"addresses\", [{}])[0].get(\"telephone_number\", \"\"),\n",
    "                result.get(\"addresses\", [{}])[0].get(\"fax_number\", \"\"),\n",
    "                result.get(\"taxonomies\", [{}])[0].get(\"code\", \"\"),\n",
    "                result.get(\"taxonomies\", [{}])[0].get(\"taxonomy_group\", \"\"),\n",
    "                result.get(\"taxonomies\", [{}])[0].get(\"desc\", \"\"),\n",
    "                result.get(\"taxonomies\", [{}])[0].get(\"license\", \"\")\n",
    "            )\n",
    "            cursor.execute(insert_query, row)\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Data successfully saved to the PostgreSQL database.\")\n",
    "    except psycopg2.DatabaseError as db_error:\n",
    "        print(f\"Database error: {db_error}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "def extract_npi_data():\n",
    "    \"\"\"Extract NPI data from the API and save it to the PostgreSQL database.\"\"\"\n",
    "    create_npi_table()\n",
    "    results = fetch_npi_data(params)\n",
    "    if results:\n",
    "        insert_npi_data(results)\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_npi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Base URL format for the ICD files\n",
    "BASE_URL = \"https://www.cms.gov/files/document/valid-icd-{}-list.xlsx\"\n",
    "\n",
    "# Database connection details (update these if needed)\n",
    "db_params = {\n",
    "    'host': 'localhost',\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'options': '-c search_path=private'\n",
    "}\n",
    "\n",
    "def check_version_exists(version):\n",
    "    \"\"\"Check if a specific version of the ICD list exists.\"\"\"\n",
    "    url = BASE_URL.format(version)\n",
    "    response = requests.head(url)\n",
    "    return response.status_code == 200, url\n",
    "\n",
    "def get_latest_icd_version():\n",
    "    \"\"\"Determine the latest available ICD version by incrementing version numbers.\"\"\"\n",
    "    version = 10  # Start checking from ICD-10\n",
    "    latest_version = None\n",
    "\n",
    "    while True:\n",
    "        exists, url = check_version_exists(version)\n",
    "        if exists:\n",
    "            latest_version = version\n",
    "            print(f\"ICD-{latest_version} is available at {url}\")\n",
    "            version += 1  # Check the next version\n",
    "        else:\n",
    "            if latest_version is not None:\n",
    "                print(f\"ICD-{version} not found. Latest available version is ICD-{latest_version}.\")\n",
    "                return latest_version, BASE_URL.format(latest_version)\n",
    "            else:\n",
    "                print(\"No valid ICD versions found.\")\n",
    "                return None, None\n",
    "\n",
    "def insert_icd_data_to_db(df):\n",
    "    \"\"\"Insert data from the DataFrame into the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS icd_codes_raw (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            code VARCHAR(10),\n",
    "            short_description TEXT,\n",
    "            long_description TEXT,\n",
    "            nf_excl TEXT\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO icd_codes_raw (code, short_description, long_description, nf_excl)\n",
    "        VALUES (%s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "        for _, row in df.iterrows():\n",
    "            cursor.execute(insert_query, (\n",
    "                row['CODE'],\n",
    "                row['SHORT DESCRIPTION (VALID ICD-10 FY2024)'],\n",
    "                row['LONG DESCRIPTION (VALID ICD-10 FY2024)'],\n",
    "                row['NF EXCL']\n",
    "            ))\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"ICD data saved to database.\")\n",
    "\n",
    "    except psycopg2.DatabaseError as db_error:\n",
    "        print(f\"Database error: {db_error}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "def download_and_process_icd_file(icd_url):\n",
    "    \"\"\"Download the ICD file from the provided URL and insert data into the database.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(icd_url)\n",
    "        if response.status_code == 200:\n",
    "            # Read the Excel file into a DataFrame\n",
    "            df = pd.read_excel(icd_url)\n",
    "\n",
    "            # Debug: Print the first few rows of the DataFrame\n",
    "            print(\"DataFrame contents:\")\n",
    "            print(df.head())\n",
    "\n",
    "            # Insert the data into the database\n",
    "            insert_icd_data_to_db(df)\n",
    "        else:\n",
    "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process the Excel file: {e}\")\n",
    "\n",
    "def extract_icd_data():\n",
    "    \"\"\"Function to extract ICD data and save it to PostgreSQL.\"\"\"\n",
    "    # Get the latest ICD version URL\n",
    "    latest_version, latest_icd_url = get_latest_icd_version()\n",
    "\n",
    "    # If a valid URL was found, download the ICD file and save metadata to the PostgreSQL database\n",
    "    if latest_icd_url:\n",
    "        download_and_process_icd_file(latest_icd_url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_icd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "db_params = {\n",
    "    'host': 'localhost',\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'port': '5432',\n",
    "    'options': '-c search_path=private'\n",
    "}\n",
    "\n",
    "def get_quarter_and_abbr(month):\n",
    "    \"\"\"Determine the quarter and its corresponding abbreviation based on the month.\"\"\"\n",
    "    if month in [1, 2, 3]:\n",
    "        return \"january\", \"JAN\"\n",
    "    elif month in [4, 5, 6]:\n",
    "        return \"april\", \"APR\"\n",
    "    elif month in [7, 8, 9]:\n",
    "        return \"july\", \"JUL\"\n",
    "    else:\n",
    "        return \"october\", \"OCT\"\n",
    "\n",
    "def get_quarterly_zip_url(year=None, month=None):\n",
    "    \"\"\"Construct the quarterly ZIP URL based on the current year and quarter.\"\"\"\n",
    "    now = datetime.now()\n",
    "    year = year or now.year\n",
    "    month = month or now.month\n",
    "\n",
    "    quarter, _ = get_quarter_and_abbr(month)\n",
    "\n",
    "    zip_url = f\"https://www.cms.gov/files/zip/{quarter}-{year}-alpha-numeric-hcpcs-file.zip\"\n",
    "    return zip_url\n",
    "\n",
    "def list_files_in_zip(zip_url):\n",
    "    \"\"\"Download the ZIP file and list all files inside.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            zip_file = BytesIO(response.content)\n",
    "\n",
    "            with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "                print(\"Files in the ZIP archive:\")\n",
    "                for file in z.namelist():\n",
    "                    print(file)\n",
    "                return z.namelist()\n",
    "        else:\n",
    "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "        return []\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"The downloaded file is not a valid ZIP file.\")\n",
    "        return []\n",
    "\n",
    "def download_and_save_to_postgres(zip_url, filename_prefix):\n",
    "    \"\"\"Download the ZIP file, extract the specific NOC file, and save the data to PostgreSQL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            zip_file = BytesIO(response.content)\n",
    "\n",
    "            with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "                file_to_extract = None\n",
    "                for file in z.namelist():\n",
    "                    if file.startswith(filename_prefix) and (file.endswith('.xls') or file.endswith('.xlsx')):\n",
    "                        file_to_extract = file\n",
    "                        break\n",
    "\n",
    "                if file_to_extract:\n",
    "                    print(f\"Extracting '{file_to_extract}' and saving data to PostgreSQL...\")\n",
    "\n",
    "                    with z.open(file_to_extract) as extracted_file:\n",
    "                        df = pd.read_excel(extracted_file)\n",
    "\n",
    "                    engine = create_engine(\n",
    "                        f'postgresql://{db_params[\"user\"]}:{db_params[\"password\"]}@{db_params[\"host\"]}:{db_params[\"port\"]}/{db_params[\"dbname\"]}?options={db_params[\"options\"]}'\n",
    "                    )\n",
    "\n",
    "                    df.to_sql('noc_codes_raw', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "                    print(f\"'{file_to_extract}' has been saved successfully to the PostgreSQL database.\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"'{filename_prefix}' not found in the ZIP file with either .xls or .xlsx extension.\")\n",
    "        else:\n",
    "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"The downloaded file is not a valid ZIP file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def get_noc_codes_filename_prefix(year, quarter_abbr):\n",
    "    \"\"\"Construct the expected NOC codes file prefix with a space between the month and year.\"\"\"\n",
    "    return f\"NOC codes_{quarter_abbr} {year}\"\n",
    "\n",
    "def extract_hcpcs_data():\n",
    "    \"\"\"Function to extract HCPCS data and save it to PostgreSQL.\"\"\"\n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "\n",
    "    quarter, quarter_abbr = get_quarter_and_abbr(month)\n",
    "    zip_url = get_quarterly_zip_url(year, month)\n",
    "\n",
    "    zip_file_list = list_files_in_zip(zip_url)\n",
    "    noc_filename_prefix = get_noc_codes_filename_prefix(year, quarter_abbr)\n",
    "\n",
    "    download_and_save_to_postgres(zip_url, noc_filename_prefix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_hcpcs_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
